{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Basic NLP operations\n",
    "\n",
    "1. Lemmatization\n",
    "1. Part-of-speech (POS) tagging\n",
    "1. Dependency parsing\n",
    "1. Rule-based pattern matching\n",
    "1. Named entity recognition (NER)\n",
    "1. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = u'''She goes for a walk every day.\n",
    "           He was going to the cinema in the evening.\n",
    "           They have already gone to work.\n",
    "           I took my coffee to go and went home.'''\n",
    "word_to_find = u'go'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    if token.lemma_ == word_to_find:\n",
    "        print(f'{token.text:<8} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = u'The sky above the port was the color of television, tuned to a dead channel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:<12} {token.pos_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = u'Bob never took Spanish at school.'\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rule-based pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = u'''Corrective actions to the previous audit findings are not implemented in a timely manner.\n",
    "           Most are not fully documented.\n",
    "           There are even some findings that the team never discussed.'''\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "negated_verb_pattern = [ {'DEP': 'neg'}, {'POS': 'VERB'} ]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('NEGATED_VERB', None, negated_verb_pattern)\n",
    "matches = matcher(doc)\n",
    "\n",
    "for rule_id, start_token, end_token in matches:\n",
    "    print(doc[start_token:end_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# {'DEP': 'advmod', 'OP': '?'}, \n",
    "# advmod - adverbial modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Token attributes - https://spacy.io/usage/linguistic-features#adding-patterns-attributes\n",
    "#    DEP - syntactic dependency\n",
    "#    OP  - quantifier (`?` means optional)\n",
    "\n",
    "# Dependency tokens - https://stackoverflow.com/a/40288324/95\n",
    "#    neg    - negation modifier\n",
    "#    advmod - adverbial modifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = u'Marek Grzenkowicz came to the Devoxx conference from Poland yesterday around 11 AM.'\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Default spaCy entity types - https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Context-free language models\n",
    "   - word2vec, GloVe, fastText\n",
    "1. Contextual models\n",
    "   - ELMo, BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')  # larger models, with word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "> He was sitting by the **river bank**.\n",
    ">\n",
    "> A **bank account** was opened for them in the morning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "1. Context-free language models - vectors trained on a text corpus from co-occurrence statistics\n",
    "1. Contextual models - word representations that are a function of the entire context of sentence or paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vector_dog = nlp.vocab[u'dog'].vector\n",
    "print(vector_dog[:42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f'length  = {len(vector_dog)}')\n",
    "print(f'min_val = {min(vector_dog)}')\n",
    "print(f'max_val = {max(vector_dog)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparing word vectors\n",
    "\n",
    "**Cosine similarity**\n",
    "![cosine similarity in 2D](images/vectors_cos_sim_600.png)\n",
    "\n",
    "**Note:** For illustrative purposes only; the values above are random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparing word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "words = ['dog', 'husky', 'cat', 'horse', 'tree', 'stone', 'bitcoin']\n",
    "\n",
    "for word in words:\n",
    "    vector_word = nlp.vocab[word].vector\n",
    "    \n",
    "    cos_sim = cosine_similarity([vector_dog], [vector_word])  # calculate cosine between two vectors\n",
    "    \n",
    "    cos_sim = cos_sim[0][0]\n",
    "    print(f'cos(dog, {word + \")\":<8} = {cos_sim:>6.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arithmetic of word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_vectors(word_vector, vocabulary, skip_words=(), num=3):\n",
    "    \"\"\"\n",
    "    Finds vectors close to `word_vector` in terms of cosine similarity, inside given `vocabulary`.\n",
    "    \"\"\"\n",
    "    result = [\n",
    "        w\n",
    "        for w in vocabulary\n",
    "        if w.prob >= -15\n",
    "        and w.is_lower\n",
    "        and any(w.vector != word_vector)\n",
    "        and w.orth_ not in skip_words\n",
    "    ]\n",
    "    result = sorted(\n",
    "        result,\n",
    "        key=lambda w: cosine_similarity([w.vector], [word_vector]),\n",
    "        reverse=True\n",
    "    )\n",
    "    return [w.orth_ for w in result[:num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "find_similar_vectors(vector_dog, nlp.vocab, num=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arithmetic of word vectors\n",
    "\\begin{equation}\n",
    "\\large{\\mathit{  V_{parent} + V_{woman} = x  }}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = nlp.vocab[u'parent'].vector + nlp.vocab[u'woman'].vector\n",
    "find_similar_vectors(x, nlp.vocab, skip_words=[u'parent', u'woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "\\large{\\mathit{  V_{seawater} - V_{salt} = x  }}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = nlp.vocab[u'seawater'].vector - nlp.vocab[u'salt'].vector\n",
    "find_similar_vectors(x, nlp.vocab, skip_words=[u'seawater', u'salt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arithmetic of word vectors\n",
    "\\begin{equation}\n",
    "\\large{\\mathit{  V_{Germany} - V_{Berlin} = x - V_{Warsaw}  }}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = nlp.vocab[u'germany'].vector - nlp.vocab[u'berlin'].vector + nlp.vocab[u'warsaw'].vector\n",
    "find_similar_vectors(x, nlp.vocab, skip_words=[u'germany', u'berlin', u'warsaw'])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
